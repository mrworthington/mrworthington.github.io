[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes From A Spreadsheet",
    "section": "",
    "text": "Gnarly Data w/ Arrow, DuckDB, + SQL\n\n\n\n\n\n\n\nggplot2\n\n\ndatabases\n\n\narrow\n\n\nsql\n\n\nduckdb\n\n\n\n\nKnowing SQL is a must for Data Scientists + other analytics professionals, but how can R users start practicing their SQL skills in a familiar environment? That‚Äôs this post!\n\n\n\n\n\n\nAug 13, 2023\n\n\nMatt Worthington\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nA Follow-Up on Open Sourcing Stuff You Know\n\n\n\n\n\n\n\nSpatial Data\n\n\nGIS\n\n\nMapping\n\n\nSharing\n\n\ntidycensus\n\n\nrdeck\n\n\n\n\nLast week, I shared about how I use R for spatial workflows on social media and got a lot of feedback. Here‚Äôs what I‚Äôve learned, plus some more spatial tips.\n\n\n\n\n\n\nOct 4, 2022\n\n\nMatt Worthington\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nA Spatial/GIS Workflow for Interactive Maps in R\n\n\n\n\n\n\n\nSpatial Data\n\n\nGIS\n\n\nMapping\n\n\nsf\n\n\nrdeck\n\n\n\n\nIn recent years, there‚Äôs been a ton of packages released in R for working with and visualizing spatial data that have eliminated my need for things like ArcGIS. Here‚Äôs how I use a few of them.\n\n\n\n\n\n\nSep 29, 2022\n\n\nMatt Worthington\n\n\n19 min\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing COVID-19 in Austin\n\n\n\n\n\n\n\nCOVID19\n\n\nPublic Health\n\n\nAustin Public Health\n\n\nI35\n\n\nAustin\n\n\nPublic Data\n\n\n\n\nAustin has been profiled numerous times throughout the pandemic as being relentless in its efforts to get good COVID-19 data, despite multiple challenges and barriers. This is the first in a set of posts where I take a look at local COVID-19 data for Austin and explore themes of risk, resilience, and disparities as it pertains to COVID-19 in Austin.\n\n\n\n\n\n\nAug 13, 2020\n\n\nMatt Worthington\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nVisualizing American School Finance\n\n\n\n\n\n\n\nSchool Finance\n\n\nPublic Education\n\n\nCharter Schools\n\n\nUS Census Bureau\n\n\n\n\nAnalyzing School Finances Revenue Breakdowns For Every State From 1993-2015\n\n\n\n\n\n\nMar 28, 2018\n\n\nMatt Worthington\n\n\n25 min\n\n\n\n\n\n\n  \n\n\n\n\nMy Testimony to The Texas Commission on Public School Finance\n\n\n\n\n\n\n\nSchool Finance\n\n\nPublic Education\n\n\nTexas Education Agency\n\n\nIndependent School Districts\n\n\nCharter Schools\n\n\n\n\nDelivered on March 19, 2018\n\n\n\n\n\n\nMar 21, 2018\n\n\nMatt Worthington\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nICYMI: This was all about racism\n\n\n\n\n\nWhile Rodriguez v. San Antonio ISD worked its way through the courts in 1968, transcripts of Civil Rights hearings revealed a deeper logic at work in San Antonio and the country that sought to disenfranchise Mexican-Americans in almost every facet of life.\n\n\n\n\n\n\nDec 6, 2017\n\n\nMatt Worthington, Bekah McKneel\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nHow San Antonio left Edgewood to fend for itself\n\n\n\n\n\nReforms of the mid- and late-20th century offered opportunities for some school districts to shore up their property tax base and create more secure funding. Edgewood, however, would not be one of these.\n\n\n\n\n\n\nNov 29, 2017\n\n\nMatt Worthington\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nHow San Antonio segregated its schools\n\n\n\n\n\nThe struggles that continue to plague Edgewood ISD are rooted in prejudice. Throughout the 20th century, those prejudices became public policy that would allocate services unevenly across the city, creating intractable inequities still visible in the city‚Äôs school districts.\n\n\n\n\n\n\nNov 22, 2017\n\n\nMatt Worthington\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nWhy Americans Have No Right To An Education\n\n\n\n\n\nHow Texas‚Äô school finance system helped perpetuate nationwide inequity.\n\n\n\n\n\n\nNov 8, 2017\n\n\nMatt Worthington\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "articles/school_finance/school-finance-analysis/school_finance_viz_us.html",
    "href": "articles/school_finance/school-finance-analysis/school_finance_viz_us.html",
    "title": "Visualizing American School Finance",
    "section": "",
    "text": "Note: This is a much older post of mine that contains an approach to coding I no longer use. I keep it up here as a reminder of the importance of growing coding skills. One of the key pieces of feedback I received from this post was to look into looping and interation. As a result, I learned about a package called {purrr}, which was a great solution for this recommendation.\nThis evening, the Ed Trust is hosting a discussion on twitter at 8PM ET on school funding inequity, so I thought I would do a code-through of a visualization I made this past summer while researching the legacy of a famous school finance Supreme Court case that ended up determining that Americans had no fundamental right to an education. In short, it shows the shares of funding over time for each of the three main sources of funding for public education in America: federal funding, state funding, and local funding (mostly property taxes)."
  },
  {
    "objectID": "articles/school_finance/school-finance-analysis/school_finance_viz_us.html#bonus-school-finance-map",
    "href": "articles/school_finance/school-finance-analysis/school_finance_viz_us.html#bonus-school-finance-map",
    "title": "Visualizing American School Finance",
    "section": "Bonus School Finance Map",
    "text": "Bonus School Finance Map\nWhile I had limited time to produce the code-through for the first map, here‚Äôs an extra map that shows how much each state‚Äôs per pupil funding compares to the national average since 1992. One note about this one is that it uses the All-Consumer series inflation index from the Bureau of Labor & Statistics for 1992, while the 1993 data onward uses the BLS Education Index."
  },
  {
    "objectID": "articles/school_finance/school-finance-analysis/school_finance_viz_us.html#photo-credit",
    "href": "articles/school_finance/school-finance-analysis/school_finance_viz_us.html#photo-credit",
    "title": "Visualizing American School Finance",
    "section": "Photo Credit",
    "text": "Photo Credit\n\nPhoto by Isabella and Louisa Fischer on Unsplash"
  },
  {
    "objectID": "articles/school_finance/school-finance-analysis/school_finance_viz_us.html#corrections",
    "href": "articles/school_finance/school-finance-analysis/school_finance_viz_us.html#corrections",
    "title": "Visualizing American School Finance",
    "section": "Corrections",
    "text": "Corrections\n\nIf you see mistakes or want to suggest changes, please create an issue on the source repository."
  },
  {
    "objectID": "articles/school_finance/school-finance-analysis/school_finance_viz_us.html#footnotes",
    "href": "articles/school_finance/school-finance-analysis/school_finance_viz_us.html#footnotes",
    "title": "Visualizing American School Finance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBefore 1992, the method of collection for the census didn‚Äôt disaggregate federal revenues from state revenues because federal dollars were technically implemented through the state. However, since 1992, the census shifted their reporting protocols for states and began requesting that each state delineate between their contributions versus what the federal government provided.‚Ü©Ô∏é\nWhile this method of collection started in 1992, our analysis will start in 1993 because that is when the Bureau of Labor & Statistics introduced their Education Inflation Index, which was meant to provide clarity on a dollar‚Äôs value in the education space.‚Ü©Ô∏é"
  },
  {
    "objectID": "articles/school_finance/edgewood_fend_for_itself/edgewood_fend_for_itself.html",
    "href": "articles/school_finance/edgewood_fend_for_itself/edgewood_fend_for_itself.html",
    "title": "How San Antonio left Edgewood to fend for itself",
    "section": "",
    "text": "Note: This story was originally published by Folo Media.\nThis is the third installment in a series of essays on the legacy of a court battle that began in San Antonio, ended in the Supreme Court, and determined that American children have no constitutional right to an education. Read the first and second essays.\nBy 1960, residential and public housing segregation had effectively consolidated San Antonio‚Äôs Hispanic residents on the West Side, where more than half lived in 18 of the city‚Äôs 92 census tracts. In nine of those tracts, nearly half of the housing was categorized by the federal government as ‚Äúdeteriorating‚Äù or ‚Äúdilapidated,‚Äù the latter of which was deemed hazardous to occupants‚Äô health and safety.\nWith unemployment rates nearly double the city‚Äôs, and with almost half of its residents without stable employment, West Side median incomes ranged from 29 to 60 percent of white incomes. Many industries limited the type of jobs black and Hispanic workers could do. As a result, nearly half the population on the West Side lived below the federal poverty line, which left a quarter of residents chronically hungry.\nInterviews from a 1968 United States Commission on Civil Rights investigation reveal widespread discrimination against Hispanic people in San Antonio. Parents and students expressed frustration as they saw another generation of bright and capable children denied the opportunity to thrive. One student at Lanier High School described college as ‚Äúan impossible dream.‚Äù\nLanier High School in San Antonio ISD would struggle with internal inequality within its district, but neighboring Edgewood ISD had been thoroughly isolated.\nEdgewood became the poster child for what writer Jonathan Kozol would call ‚ÄúSavage Inequalities‚Äù in his 1991 book of the same name. It was not only the poorest district in Texas at the time, but the poorest district in America‚Äôs 16 largest cities.\nWhat is most troubling is that 70 years prior, some in Texas had predicted exactly this."
  },
  {
    "objectID": "articles/school_finance/edgewood_fend_for_itself/edgewood_fend_for_itself.html#acknowledgments",
    "href": "articles/school_finance/edgewood_fend_for_itself/edgewood_fend_for_itself.html#acknowledgments",
    "title": "How San Antonio left Edgewood to fend for itself",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was produced during my time at Folo Media during 2017. Folo Media was a nonprofit newsroom focused on the challenges of inequity and neighborhood segregation in San Antonio, Texas. The newsroom ran from the spring of 2017 through early 2018, and during that span published more than 100 stories covering a range of issues, such as: housing, education, racial segregation, non-profit solutions, and much more.\nIf you are interested in following the work of those who helped lead Folo Media‚Äôs work, I encourage you to keep up with the American Journalism Project, the H.E.Butt Foundation‚Äôs Echoes Magazine, and the San Antonio Heron."
  },
  {
    "objectID": "articles/school_finance/edgewood_fend_for_itself/edgewood_fend_for_itself.html#author-contributions",
    "href": "articles/school_finance/edgewood_fend_for_itself/edgewood_fend_for_itself.html#author-contributions",
    "title": "How San Antonio left Edgewood to fend for itself",
    "section": "Author Contributions",
    "text": "Author Contributions\n\nBekah McKneel (formerly of Folo Media) contributed to this report. She is awesome and I would encourage you to check out her work."
  },
  {
    "objectID": "articles/rstats/sharing-rdeck-follow-up/index.html",
    "href": "articles/rstats/sharing-rdeck-follow-up/index.html",
    "title": "A Follow-Up on Open Sourcing Stuff You Know",
    "section": "",
    "text": "Last week, I shared a semi-lengthy write-up of one of my favorite spatial packages in R. In part, I‚Äôd been inspired by an experience I had at RStudio/Posit Conference where I realized part of why I don‚Äôt share is because I thought I didn‚Äôt have anything worth sharing. For fear of rehashing the entire post, here‚Äôs the TLDR from last post: I shared about why rdeck is great and got a lot of great feedback.\nPart of that feedback was an interesting request that I‚Äôd actually been toying around with the day before. Jon Harmon asked if I‚Äôd share what I‚Äôd learned by deciding to share. Well, let‚Äôs get into that.\n\nA follow-up blog about the things you learned by posting this would be both meta and cool üòÅ‚Äî Jon Harmon (he/him) (@JonTheGeek) October 1, 2022"
  },
  {
    "objectID": "articles/rstats/sharing-rdeck-follow-up/index.html#some-background-on-this-post",
    "href": "articles/rstats/sharing-rdeck-follow-up/index.html#some-background-on-this-post",
    "title": "A Follow-Up on Open Sourcing Stuff You Know",
    "section": "",
    "text": "Last week, I shared a semi-lengthy write-up of one of my favorite spatial packages in R. In part, I‚Äôd been inspired by an experience I had at RStudio/Posit Conference where I realized part of why I don‚Äôt share is because I thought I didn‚Äôt have anything worth sharing. For fear of rehashing the entire post, here‚Äôs the TLDR from last post: I shared about why rdeck is great and got a lot of great feedback.\nPart of that feedback was an interesting request that I‚Äôd actually been toying around with the day before. Jon Harmon asked if I‚Äôd share what I‚Äôd learned by deciding to share. Well, let‚Äôs get into that.\n\nA follow-up blog about the things you learned by posting this would be both meta and cool üòÅ‚Äî Jon Harmon (he/him) (@JonTheGeek) October 1, 2022"
  },
  {
    "objectID": "articles/rstats/sharing-rdeck-follow-up/index.html#general-stuff-i-learned",
    "href": "articles/rstats/sharing-rdeck-follow-up/index.html#general-stuff-i-learned",
    "title": "A Follow-Up on Open Sourcing Stuff You Know",
    "section": "General Stuff I Learned",
    "text": "General Stuff I Learned\nThe R Community is Freaking Amazing\nThe first thing I learned was really a reminder, which is how supportive and inclusive the #rstats community is. Not kidding. I think, at times, I came to believe I didn‚Äôt have a place in contributing to the greater R community simply because I didn‚Äôt grow up as a quant-person. In fact, I grew up really poor and, even though I went to college, I majored in English1.\nSo, part of the thing that has kept me from sharing is thinking I didn‚Äôt qualify. It‚Äôs not entirely crazy to feel that way, TBH. Just within the #rstats community, there‚Äôs people like Hadley Wickham, Yihui Xie, Julia Silge, Jenny Bryan, Thomas Lin Pedersen, Alison Presmanes Hill, Mine √áetinkaya-Rundel, Edzer Pebesma, Kyle Walker, Bob Rudis, David Robinson, Danielle Navarro, Gabriela de Queiroz, and Tom Mock.\nSo, if you don‚Äôt come from a quant background like me, it‚Äôs not entirely crazy to feel like a fish out of water when you‚Äôre learning from these folks and, let‚Äôs be clear, I‚Äôve learned a ton from these folks. They are all unique talents and most have got some kind of formal education or training that‚Äôs quant-oriented.\nAnd, yet‚Ä¶ part of the incredible success of R as a language is how ridiculously inclusive everyone is (folks above included). And that act of inclusion, IMO, is sharing. It‚Äôs all these people with incredible knowledge figuring out how to share it and make it accessible. It‚Äôs doing the work of translating knowledge to anyone that wants to learn.\nAnyway, I was reminded of just how awesome the R community is online. And I learned that sharing is part of the way you join what makes R awesome, even if you don‚Äôt think you‚Äôd make the cut in a game of pick-up.\nOthers Learn When You Share\nTo that end, I also learned people learn from you. As a former middle school teacher, it was slightly reassuring to be like, ‚ÄúOh yeah, I can teach stuff effectively.‚Äù üòÇ Not only were people online learning from the post, but one of my colleagues who just started learning R used it as a way to get their feet wet with Quarto/Rmarkdown docs. And, to my surprise, they were making their own version of the rdeck map I made in the last post!\nYou Learn A Ton When You Share\nThe last thing I learned was more about the workflow I described. Anthony North and Miles McBain both chimed in with really great bits of information that either clarified or added to what I knew about rdeck. So, in the spirit of sharing, here‚Äôs some of that stuff. But instead of using the same map I used last time, I thought it might be good to place the other things I learned in the context of tidycensus."
  },
  {
    "objectID": "articles/rstats/sharing-rdeck-follow-up/index.html#map-stuff-i-learned",
    "href": "articles/rstats/sharing-rdeck-follow-up/index.html#map-stuff-i-learned",
    "title": "A Follow-Up on Open Sourcing Stuff You Know",
    "section": "Map Stuff I Learned",
    "text": "Map Stuff I Learned\nFirst of all, {tidycensus}\n\nI didn‚Äôt actually learn about tidycensus through sharing the last post, but it‚Äôs another incredible asset in my own spatial workflow. Part of that is because I went to grad school for public policy in the US and working with data from the US Census Bureau was‚Ä¶\n\nSomething I wanted to be good at\nA slight nightmare to work with. Especially if it involved making maps or doing any kind of spatial analysis.\n\nIt‚Äôs hard to overstate how much Kyle Walker‚Äôs tidycensus addressed both of those in the best way possible. Let me show you how. First things first, let‚Äôs load the packages.\n\nThe Code | Package Setup```{r load-packages}\n#| code-summary: \"**The Code** | Package Setup\"\nlibrary(tidyverse)     # Loads core tidyverse tools\nlibrary(tidycensus)    # Loads tools to pull census data\nlibrary(viridis)       # Loads color palettes I use w/ {rdeck}\nlibrary(rdeck)         # Loads rdeck library\nlibrary(sf)            # Loads core spatial tools\noptions(tigris_use_cache = TRUE) # Caches data we pull from tidycensus to speed up recalls\n```\n\n\nNow that tidycensus is loaded, we can start pulling census data as long as you‚Äôve already got your API Key from the US Census. Fortunately, Kyle‚Äôs got a great tutorial for this on his tidycensus website. Using the spatial example on his website, I‚Äôm going to download some tract-level census data from the latest 2020 ACS Estimates and then get the spatial data to go along with it. The exception here is that I‚Äôm going to focus on Travis County (Austin), where I live, instead of Harris County (Houston). No shade to Houston, though. The humidity‚Äôs so thick there it feels like someone‚Äôs always breathing on you during the summer, but I otherwise love Houston and think it‚Äôs an amazing city. Anyway, data‚Ä¶\n\nThe Code | {tidycensus} Call```{r get-acs-spatial}\n#| code-fold: show\n#| code-summary: \"**The Code** | {tidycensus} Call\"\n\ntravis &lt;- get_acs(\n  state = \"TX\",\n  county = \"Travis\",\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  geometry = TRUE,\n  year = 2020\n) |&gt; \n  st_transform(crs = st_crs(4326))\n```\n\n\nAlright, so what happened? Well, we got data for every single census tract‚Äôs Median Household Income in Travis County. A snapshot of that data is shown in the output below, but wait‚Ä¶ what‚Äôs that st_transform() thing about in the chunk above?\nWell, if you take a look at the bold/highlighted text below, you notice that our CRS reads ‚ÄúWGS 84‚Äù. I‚Äôve done that because we‚Äôre going to map polygons and rdeck requires polygons to have a CRS of ‚ÄúWGS 84‚Äù and not ‚ÄúNAD 83‚Äù (which is what comes out of tidycensus).  You don‚Äôt need to know the technical details to use it, but you‚Äôre more than welcome to learn more about it.CRS = Coordinate Reference System\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -97.77649 ymin: 30.18394 xmax: -97.644 ymax: 30.32954\nGeodetic CRS:  WGS 84\n        GEOID                                     NAME   variable estimate\n1 48453002201 Census Tract 22.01, Travis County, Texas B19013_001    57803\n2 48453000304  Census Tract 3.04, Travis County, Texas B19013_001    80000\n3 48453001307 Census Tract 13.07, Travis County, Texas B19013_001    76009\n4 48453002105 Census Tract 21.05, Travis County, Texas B19013_001    40811\n5 48453002413 Census Tract 24.13, Travis County, Texas B19013_001    45462\n6 48453000801  Census Tract 8.01, Travis County, Texas B19013_001    83958\nSo, now, I‚Äôve got my spatial data and want to map it. Well, one thing I thought would be great is trying to replicate a ggplot2-style map. I know I could use ggiraph (another amazing package I‚Äôll write about at some point), but I wanted to make something that feels like a ggplot2 map made with geom_sf(), but interactive. I also learned that rdeck was partly inspired by ggplot2. To get that feel with rdeck, you can set the map_style argument to NULL and controller=FALSE get something similar to that ggplot2-style map.\n\nThe Code | {tidycensus} ggplot-style Map```{r tidycensus-map}\n#| column: page\n#| fig-height: 7\n#| code-summary: \"**The Code** | {tidycensus} ggplot-style  Map\"\n\ntravis_map &lt;- rdeck(map_style = NULL,\n      initial_bounds = travis,\n      controller = FALSE,\n      theme = \"light\") |&gt; \n  add_polygon_layer(data = travis,\n                    visibility_toggle = TRUE,\n                    name = \"Median Household Income (by Census Tract)\",\n                    opacity = 0.9,\n                    get_polygon = geometry,\n                    tooltip = estimate,\n                    pickable = TRUE,\n                    auto_highlight = TRUE,\n                    get_fill_color = scale_color_linear(\n                      col = estimate,\n                      palette = magma(10)\n                    ))\n\ntravis_map\n```\n\n\n\n\n\n\nA Simpler Bounding Box Method\nIn the last map, I talked about using a fairly complicated function to get a bounding box. One of the things Anthony clarified is that you can just pass a spatial object of st_bbox, sf, or sfc class. This means we can just pass the travis object we created with tidycensus and pass it to the initial_bounds argument (second line in the code below). Amazing.\ntravis_map &lt;- rdeck(map_style = NULL,\n      initial_bounds = travis,\n      controller = FALSE,\n      theme = \"light\") |&gt; \n  add_polygon_layer(data = travis,\n                    visibility_toggle = TRUE,\n                    name = \"Median Household Income (by Census Tract)\",\n                    opacity = 0.9,\n                    get_polygon = geometry,\n                    tooltip = estimate,\n                    pickable = TRUE,\n                    auto_highlight = TRUE,\n                    get_fill_color = scale_color_linear(\n                      col = estimate,\n                      palette = magma(10)\n                    ))\nAnnotating{rdeck} with the {rdeck} editor\nOne really amazing thing I didn‚Äôt realize is that rdeck has an editor built-in. The editor can be turned on by setting editor=TRUE in the main rdeck() function. Miles McBain pointed out that you can use this with shiny for filtering spatial data and any linked data in-app. That sounds very useful, but I also think you can use it for creating custom annotations to spatial maps if you want to call out a particular area on maps you regularly make.\nThis sounds simple enough, but spatial annotations can be pretty difficult without a tool like this where you can see the area you‚Äôre trying to highlight on a map. Take a look at the GIF below to see how it works.\n\n\nDrawing with the built-in {rdeck} editor\n\nIn short, you click on the tool you want (in my case a polygon), then you can draw with the tool, and once you‚Äôre done, rdeck saves it as a ‚Äú.geojson‚Äù file which can be read back into R using read_sf() and now you can add that spatial object onto your map as an annotation. The code chunk below does just that. Pretty great, huh?\n\nThe Code | Annotated Map```{r annotated-map}\n#| column: page\n#| fig-height: 7\n#| code-summary: \"**The Code** | Annotated Map\"\n\ncustom_shape &lt;- read_sf(\"rdeck.geojson\")\n\ntravis_map |&gt; \n  add_polygon_layer(data = custom_shape,\n                    name = \"Highlighted Area\",\n                    opacity = 0.1,\n                    get_polygon = geometry,\n                    get_fill_color = \"#5da5da\")\n```\n\n\n\n\n\n\nAnyway, enjoy digging more into rdeck (and tidycensus). Hope you find new stuff and write about it! Doesn‚Äôt even have to be a blog. Could just be twitter."
  },
  {
    "objectID": "articles/rstats/sharing-rdeck-follow-up/index.html#acknowledgements",
    "href": "articles/rstats/sharing-rdeck-follow-up/index.html#acknowledgements",
    "title": "A Follow-Up on Open Sourcing Stuff You Know",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nPhoto by Devin Avery on Unsplash"
  },
  {
    "objectID": "articles/rstats/sharing-rdeck-follow-up/index.html#footnotes",
    "href": "articles/rstats/sharing-rdeck-follow-up/index.html#footnotes",
    "title": "A Follow-Up on Open Sourcing Stuff You Know",
    "section": "Footnotes",
    "text": "Footnotes\n\nFun Fact. My brother was the quant person. He has an actual degree in mathematics.‚Ü©Ô∏é"
  },
  {
    "objectID": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html",
    "href": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html",
    "title": "Gnarly Data w/ Arrow, DuckDB, + SQL",
    "section": "",
    "text": "At work, we‚Äôre gearing up to start using SQL databases and, admittedly, it‚Äôs been a while since I‚Äôve used SQL. While I can write dplyr in my sleep (which is very similar), I‚Äôve been wanting to get some practice rounds in to refresh my SQL familiarity with interesting data. In part, it‚Äôs for me but we also have folks on our team that are learning how to code and some will be using SQL for the first time. Coincidentally, Spatial Data Wizard Kyle Walker, posted this on twitter which caught my eye:\n\nMany data scientists find that SQL and databases are crucial skills on the job. But how do you learn these skills coming from the #rstats world?Ch11 of my book gets you practice setting up, querying, and analyzing a 9m record database from @ipums: https://t.co/0hhD3f30Wo pic.twitter.com/0GW1tdAQYY‚Äî Kyle Walker (@kyle_e_walker) August 2, 2023"
  },
  {
    "objectID": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html#why-this-post",
    "href": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html#why-this-post",
    "title": "Gnarly Data w/ Arrow, DuckDB, + SQL",
    "section": "",
    "text": "At work, we‚Äôre gearing up to start using SQL databases and, admittedly, it‚Äôs been a while since I‚Äôve used SQL. While I can write dplyr in my sleep (which is very similar), I‚Äôve been wanting to get some practice rounds in to refresh my SQL familiarity with interesting data. In part, it‚Äôs for me but we also have folks on our team that are learning how to code and some will be using SQL for the first time. Coincidentally, Spatial Data Wizard Kyle Walker, posted this on twitter which caught my eye:\n\nMany data scientists find that SQL and databases are crucial skills on the job. But how do you learn these skills coming from the #rstats world?Ch11 of my book gets you practice setting up, querying, and analyzing a 9m record database from @ipums: https://t.co/0hhD3f30Wo pic.twitter.com/0GW1tdAQYY‚Äî Kyle Walker (@kyle_e_walker) August 2, 2023"
  },
  {
    "objectID": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html#using-arrow-duckdb-for-sql-practice",
    "href": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html#using-arrow-duckdb-for-sql-practice",
    "title": "Gnarly Data w/ Arrow, DuckDB, + SQL",
    "section": "Using Arrow + DuckDB for SQL Practice",
    "text": "Using Arrow + DuckDB for SQL Practice\nSo I got the tutorial going and right about the time I started importing the IPUMS data into my PostgreSQL with DBeaver, I realized that loading it would take 2 hours and opted for something that could let me play with the data in SQL much faster.\nEnter Arrow + DuckDB. If you haven‚Äôt heard of Arrow or DuckDB, they‚Äôre fairly new tools to R‚Äôs world of wizardry. They work across multiple languages, but I‚Äôll be using their R docs for now because they‚Äôre really good.\nSo what are they? Well, Arrow bills itself as making it easy to work with ‚Äúin-memory and larger-than-memory data‚Äù while DuckDB is an ‚Äúin-process SQL OLAP database management system‚Äù. In layman‚Äôs terms, their purpose is for handling ridiculously large data within your local environment that may otherwise only be possible using larger, more involved database management systems. Together, they ‚Äúare capable of executing on data without fully loading it from disk‚Äù.\nThat last part is key for me right now, because I don‚Äôt have 2 hours to load the export from Kyle‚Äôs tutorial in PostgreSQL before practicing. Granted, the data is pretty massive at 126,567,125 rows and 17 columns. I can‚Äôt even load the 10GB CSV extract on my M1 MacBook Pro with read_csv(). If I try, RStudio‚Äôs console casually starts out like this‚Ä¶\nindexing usa_00002.csv [========================================================--] 505.37MB/s, eta:  1s\nThen, it just kind of hangs. After several minutes, RStudio finally taps out and says:\n&gt; ipums_csv &lt;- read_csv(\"usa_00002.csv\")\nError: vector memory exhausted (limit reached?)    \nSo how do we load this into a database that will let us run SQL queries on Kyle‚Äôs gigantic tutorial data? Fortunately, Arrow + DuckDB‚Äôs R Packages let us easily read our CSV file in as an Arrow Table, explore it a bit, and then load it as a database that will allow us to run SQL queries on it. 1\n\n\n\n\n\n\nFor context, I‚Äôm going to use a parquet format (which is much lighter than CSV), but instead of read_parquet(), you can use read_csv_arrow() on the CSV export from IPUMS to get the same results. It will just result in slightly slower load times.\n\n\n\n\n\n\n\nShow the code | Build A SQL Database with Arrow + DuckDBlibrary(arrow)\nlibrary(duckdb)\n\n## Reads Our IPUMS Parquet into An Arrow Table\nipums_db &lt;- arrow::read_parquet(\"raw_data/ipums_export.parquet\", as_data_frame = FALSE)\n\n# Establish A Database Connection with DuckDB\ncon &lt;- dbConnect(duckdb::duckdb())\n\n# Registers arrow table as a DuckDB view\narrow::to_duckdb(ipums_db, table_name = \"ipums_db\", con = con)\n\n# Source:   table&lt;ipums_db&gt; [?? x 17]\n# Database: DuckDB 0.8.1 [root@Darwin 22.2.0:R 4.3.1/:memory:]\n    YEAR SAMPLE SERIAL CBSERIAL  HHWT CLUSTER STATEFIP STRATA    GQ PERNUM PERWT\n   &lt;int&gt;  &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt; &lt;dbl&gt;\n 1  1850 185001    101       NA    97 1.85e12        1 1.10e8     1      1    97\n 2  1850 185001    101       NA    97 1.85e12        1 1.10e8     1      2    97\n 3  1850 185001    101       NA    97 1.85e12        1 1.10e8     1      3    97\n 4  1850 185001    101       NA    97 1.85e12        1 1.10e8     1      4    97\n 5  1850 185001    101       NA    97 1.85e12        1 1.10e8     1      5    97\n 6  1850 185001    101       NA    97 1.85e12        1 1.10e8     1      6    97\n 7  1850 185001    101       NA    97 1.85e12        1 1.10e8     1      7    97\n 8  1850 185001    101       NA    97 1.85e12        1 1.10e8     1      8    97\n 9  1850 185001    101       NA    97 1.85e12        1 1.10e8     1      9    97\n10  1850 185001    201       NA    97 1.85e12        1 1.10e8     1      1    97\n# ‚Ñπ more rows\n# ‚Ñπ 6 more variables: SEX &lt;int&gt;, AGE &lt;int&gt;, MARST &lt;int&gt;, LIT &lt;int&gt;,\n#   VERSIONHIST &lt;int&gt;, HISTID &lt;chr&gt;"
  },
  {
    "objectID": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html#exploratory-analysis-with-sql-queries-on-duckdb",
    "href": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html#exploratory-analysis-with-sql-queries-on-duckdb",
    "title": "Gnarly Data w/ Arrow, DuckDB, + SQL",
    "section": "Exploratory Analysis with SQL Queries On DuckDB",
    "text": "Exploratory Analysis with SQL Queries On DuckDB\nGenerally, I know what‚Äôs in this data export. It‚Äôs a complete-count census file from 1910. It has features that tell us about the population (like age, sex, state, marital status, literacy, etc.) but now that it‚Äôs loaded, I want to do some high-level exploration of what‚Äôs in the 126 million rows and 17 columns. As an eyebrow raiser, I‚Äôll note that Kyle‚Äôs tutorial says the export should have 92 million rows. So what gives?\nPreviewing Our Data\nFor starters, I need to recall all of the features we exported so I know exactly what I‚Äôm working with before I identify why our sample is different than the one Kyke reported. It‚Äôs technically available above, but I want a small look at just 10 rows, so I‚Äôm gonna pull that sample and preview all table columns:\n\nShow the code | Previewing 10 Observations from our DBSELECT * FROM ipums_db LIMIT 10\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nSAMPLE\nSERIAL\nCBSERIAL\nHHWT\nCLUSTER\nSTATEFIP\nSTRATA\nGQ\nPERNUM\nPERWT\nSEX\nAGE\nMARST\nLIT\nVERSIONHIST\nHISTID\n\n\n\n1850\n185001\n101\nNA\n97\n1.85e+12\n1\n110100100\n1\n1\n97\n1\n43\nNA\n4\nNA\nNA\n\n\n1850\n185001\n101\nNA\n97\n1.85e+12\n1\n110100100\n1\n2\n97\n2\n38\nNA\n4\nNA\nNA\n\n\n1850\n185001\n101\nNA\n97\n1.85e+12\n1\n110100100\n1\n3\n97\n2\n17\nNA\n0\nNA\nNA\n\n\n1850\n185001\n101\nNA\n97\n1.85e+12\n1\n110100100\n1\n4\n97\n2\n14\nNA\n0\nNA\nNA\n\n\n1850\n185001\n101\nNA\n97\n1.85e+12\n1\n110100100\n1\n5\n97\n1\n11\nNA\n0\nNA\nNA\n\n\n1850\n185001\n101\nNA\n97\n1.85e+12\n1\n110100100\n1\n6\n97\n2\n8\nNA\n0\nNA\nNA\n\n\n1850\n185001\n101\nNA\n97\n1.85e+12\n1\n110100100\n1\n7\n97\n2\n7\nNA\n0\nNA\nNA\n\n\n1850\n185001\n101\nNA\n97\n1.85e+12\n1\n110100100\n1\n8\n97\n2\n5\nNA\n0\nNA\nNA\n\n\n1850\n185001\n101\nNA\n97\n1.85e+12\n1\n110100100\n1\n9\n97\n2\n1\nNA\n0\nNA\nNA\n\n\n1850\n185001\n201\nNA\n97\n1.85e+12\n1\n110100100\n1\n1\n97\n1\n38\nNA\n4\nNA\nNA\n\n\n\n\n\nFiltering On Columns\nSingle Column\nOkay, so a quick look at our sample reveals we accidentally grabbed more than the 1910 data from Kyle‚Äôs tutorial, which may explain the discrepancy in sample sizes. Let‚Äôs filter it down to match Kyle‚Äôs reported figure of 92 million rows and count it to verify since ours is 126 million.\n\nShow the code | Counting All Observations From 1910SELECT count(*) FROM ipums_db\n  WHERE YEAR = 1910\n\n\n1 records\n\ncount_star()\n\n\n92966771\n\n\n\n\nIt‚Äôs a match! So now, let‚Äôs re-look at a small sample with the query below, using only 1910 data. When we run it, it‚Äôs easy to see that some features now have data (like marital status) whereas they didn‚Äôt before. Likely, this may have been because the 1850 complete-count census had not included that question. Another big difference between the two is the HHWT column, which tells us how many households are represented in this count. For the 1850 census, they reported only accounting for 97% of US Households in their complete-count data. For the 1910 census, we see that 100% of US Households are accounted for.\n\nShow the code | Filtering on A Single ColumnSELECT * FROM ipums_db\n  WHERE YEAR = 1910\n  LIMIT 10\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nSAMPLE\nSERIAL\nCBSERIAL\nHHWT\nCLUSTER\nSTATEFIP\nSTRATA\nGQ\nPERNUM\nPERWT\nSEX\nAGE\nMARST\nLIT\nVERSIONHIST\nHISTID\n\n\n\n1910\n191002\n101\nNA\n100\n1.91e+12\n1\n110100100\n1\n1\n100\n1\n23\n1\n1\nNA\nNA\n\n\n1910\n191002\n101\nNA\n100\n1.91e+12\n1\n110100100\n1\n2\n100\n2\n25\n1\n4\nNA\nNA\n\n\n1910\n191002\n102\nNA\n100\n1.91e+12\n1\n110100100\n1\n1\n100\n2\n30\n2\n4\nNA\nNA\n\n\n1910\n191002\n201\nNA\n100\n1.91e+12\n1\n110100100\n1\n1\n100\n1\n35\n1\n4\nNA\nNA\n\n\n1910\n191002\n201\nNA\n100\n1.91e+12\n1\n110100100\n1\n2\n100\n2\n35\n1\n4\nNA\nNA\n\n\n1910\n191002\n201\nNA\n100\n1.91e+12\n1\n110100100\n1\n3\n100\n1\n8\n6\n0\nNA\nNA\n\n\n1910\n191002\n201\nNA\n100\n1.91e+12\n1\n110100100\n1\n4\n100\n2\n5\n6\n0\nNA\nNA\n\n\n1910\n191002\n201\nNA\n100\n1.91e+12\n1\n110100100\n1\n5\n100\n2\n3\n6\n0\nNA\nNA\n\n\n1910\n191002\n201\nNA\n100\n1.91e+12\n1\n110100100\n1\n6\n100\n1\n1\n6\n0\nNA\nNA\n\n\n1910\n191002\n301\nNA\n100\n1.91e+12\n1\n110100100\n1\n1\n100\n1\n69\n1\n4\nNA\nNA\n\n\n\n\n\nMultiple Columns\nDeviating a bit from Kyle‚Äôs tutorial a bit, I‚Äôm going to do an aside and explore age distributions in Texas for this example. To do that, we can start by filtering for Texans within the 1910 census sample using the STATEFIP Feature, which uses ‚ÄúFIPS‚Äù codes to classify states. Texas is number 48.\n\n\nNow, I did count this sample outside of the tutorial (just to keep it brief) and the count for Texas in 1910 was around 3.9 million. Just over 100 years later, we‚Äôre now over 30 million. That‚Äôs pretty wild.\n\nShow the code | Filtering on Multiple ColumnsSELECT YEAR, SAMPLE, STATEFIP, SEX, AGE, LIT, MARST FROM ipums_db\n  WHERE YEAR = 1910\n  AND STATEFIP = 48\n  LIMIT 10\n\n\nDisplaying records 1 - 10\n\nYEAR\nSAMPLE\nSTATEFIP\nSEX\nAGE\nLIT\nMARST\n\n\n\n1910\n191002\n48\n1\n55\n4\n1\n\n\n1910\n191002\n48\n2\n51\n4\n1\n\n\n1910\n191002\n48\n2\n27\n4\n6\n\n\n1910\n191002\n48\n2\n16\n4\n6\n\n\n1910\n191002\n48\n1\n26\n1\n1\n\n\n1910\n191002\n48\n2\n24\n4\n1\n\n\n1910\n191002\n48\n2\n7\n0\n6\n\n\n1910\n191002\n48\n2\n5\n0\n6\n\n\n1910\n191002\n48\n2\n2\n0\n6\n\n\n1910\n191002\n48\n1\n28\n4\n1\n\n\n\n\n\nGrouping + Summary Operations\nOkay, so now we‚Äôve previewed our Texas data, let‚Äôs build off our query above and use GROUP BY on the AGE and SEX variables. Interestingly, our dataset says there were a couple of folks who lived to see 120 and one woman who lived to 130 in 1910. Now, I think these folks were lying about their birth certificates, but whatever. I‚Äôm gonna set that aside for now and work with it.\n\nShow the code | Counting Texans by Age + Gender in 1910SELECT SEX, AGE, COUNT(*) AS \"count\" \n  FROM ipums_db\n  WHERE YEAR = 1910 AND STATEFIP = 48\n  GROUP BY AGE, SEX\n  ORDER BY AGE\n\n\nDisplaying records 1 - 10\n\nSEX\nAGE\ncount\n\n\n\n2\n0\n56893\n\n\n1\n0\n58459\n\n\n1\n1\n50299\n\n\n2\n1\n47805\n\n\n2\n2\n55398\n\n\n1\n2\n57393\n\n\n1\n3\n56565\n\n\n2\n3\n55604\n\n\n1\n4\n55697\n\n\n2\n4\n54196\n\n\n\n\n\nLogical Operations\nSo, maybe you‚Äôre wondering how I knew our 130yo Texan was a woman. And it‚Äôs because the IPUMS has a code book, which articulates the classification system used for certain features. In this case, it‚Äôs 1 for Male and 2 for Female. That‚Äôs all good and well, but I don‚Äôt think it‚Äôs readable and it‚Äôd be a lot better if we just reclassified these codes. We can do this by building off our query with CASE & WHEN\n\nShow the code | Reclassifying from Coded Gender VarsSELECT AGE as \"age\",\n  CASE WHEN SEX = 1 then 'Male'\n       WHEN SEX = 2 then 'Female'\n       ELSE 'Not Specified'\n  END AS gender,\n  COUNT(*) AS \"count\"\n  FROM ipums_db\n  WHERE YEAR = 1910 AND STATEFIP = 48\n  GROUP BY age, gender\n  ORDER BY age\n\n\nVisualizing Our Query\nFrom here, I may want to take this data and visualize it. Unfortunately, SQL really isn‚Äôt the best place to do that, but I can just pass this data to R or Python with Quarto and start going to work on some nice charts. In my case, I‚Äôve tried to line up these two distributions and compare them by decade and gender for a simple visual comparison. While imagining the visualization in my mind, I decided to do some extra transformation on the 253 rows I pulled. It was simple enough to quickly do that within R and pass it to our chart for the comparison.\nAt a quick glance, we can some similarities in population size before age 30. Once 30 hits, the gaps start to widen. We could explore that more, but that‚Äôs not this post! So I‚Äôll stop elaborating on this visualization for now.\n\nShow the code | R Code to Visualize Our Query Datalibrary(tidyverse)\n\nchart_query_df &lt;- as_query |&gt; \n  mutate(decade = cut(age, breaks = c(seq(0, max(age) + 9, by = 10), Inf), \n                      right = FALSE, labels = FALSE, include.lowest = TRUE),\n         decade = decade-1,\n         decade_lbl = if_else(decade==0,\n                              paste0(decade, \"-\", decade+1, \"0\"),\n                              paste0(decade, \"0-\", decade+1, \"0\"))) |&gt; \n  group_by(decade, decade_lbl, gender) |&gt; \n  summarize(count = sum(count))\n\nchart_query_df |&gt; \n  ggplot() +\n  aes(x=reorder(decade_lbl, decade), y = count, fill = gender) +\n  geom_col(alpha = 0.8, position = position_dodge()) +\n  geom_col(data = chart_query_df |&gt; filter(decade &gt;= 30), position = position_dodge(),) + \n  scale_y_continuous(labels = scales::label_comma(scale = .001, suffix = \"K\"),\n                     n.breaks = 6) +\n  scale_fill_manual(values = c(Male = \"#005f86\", Female = \"#bf5700\")) + \n  mrworthingtonR::theme_mrw(base_family = \"Graphik\",\n                            subtitle_size = 18) +\n  theme(plot.title = ggtext::element_markdown(margin = ggplot2::margin(b = 5), hjust = 0),\n        panel.grid.major.y = element_line(color = alpha(\"#8d8d8d\", 0.2), linetype = 1)) +\n  labs(title = \"In 1910, Texas &lt;span style='color:#005f86'&gt;Men&lt;/span&gt; Seemed to Live Longer than &lt;span style='color:#bf5700'&gt;Women&lt;/span&gt; After Age 30\",\n       subtitle = \"Distributions Among Texan Ages, by Gender | 1910 Complete US Census Count\",\n       y = \"Population Count\",\n       x = \"Age Group\",\n       caption = \"Source: IPUMS 1910 Complete-Count US Census File (CSV)\")"
  },
  {
    "objectID": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html#concluding-thoughts",
    "href": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html#concluding-thoughts",
    "title": "Gnarly Data w/ Arrow, DuckDB, + SQL",
    "section": "Concluding Thoughts",
    "text": "Concluding Thoughts\nAnd that‚Äôs it for me. There‚Äôs obviously more I could do more with the chart itself and there‚Äôs far more questions to ask about the underlying data (which Kyle does on his tutorial), but I‚Äôm going to pause here since this was a post to show how you can use R, Arrow, + DuckDB to wrangle gigantic volumes of data and derive insights with SQL queries! So, if you were looking for a quick database environment going so you can practice SQL, you can use this same approach with virtually any other data. Just load it into a DuckDB and start running SQL query chunks within a Quarto Doc!\nIf you have any questions, revisions, or suggestions, just shoot me a message on twitter or X or whatever it is now."
  },
  {
    "objectID": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html#acknowledgments",
    "href": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html#acknowledgments",
    "title": "Gnarly Data w/ Arrow, DuckDB, + SQL",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nPhoto by Melissa Walker Horn on Unsplash"
  },
  {
    "objectID": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html#footnotes",
    "href": "articles/rstats/gnarly-data-arrow-sql-duckdb/index.html#footnotes",
    "title": "Gnarly Data w/ Arrow, DuckDB, + SQL",
    "section": "Footnotes",
    "text": "Footnotes\n\nFor a good tutorial on this approach, visit this page on DuckDB‚Äôs blog.‚Ü©Ô∏é"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Originally from San Antonio, TX, I live in Austin with my wife, Jess, and our family. Currently, I am the Associate Director of Data Science + Analytics at Longevity Partners. In addition, I serve as a commission member for the City of Austin‚Äôs Early Childhood Council and as the Vice-President of the Del Valle Commununity Coalition.\nProfessional Background: Prior to now, I have led data work for the LBJ School of Public Affairs‚Äôs Data Initiatives, Texas 2036 and the United Way for Greater Austin. Before my career in Data Science and Analytics, I worked as an educator for several years with KIPP Austin Public Schools and DC Public Schools in Washington, DC. At KIPP, I served as the Digital Learning Coordinator for the Austin region. In DC, I taught at MacFarland Middle School in DCPS where I served as a Literacy Intervention Specialist, Special Education Teacher, and IT Coordinator. When I moved to the DCPS Central Office, I worked as the Manager of Technology Strategy in the Office of the Chief Operating Officer.\nEducation Background: I hold Master‚Äôs degrees in Public Affairs from the LBJ School of Public Affairs at the University of Texas at Austin (Austin, TX) and in Education from George Mason University (Fairfax, VA). In my undergraduate life, I studied English at Abilene Christian University in Abilene, TX.\nResearch & Personal Interests: Having grown up in poverty and fatherless, my policy interests explore the intersections of poverty, public health, and public education. My non-policy interests center around my wife and our family, our friends, our church, getting outdoors, cooking, reading, podcasts, and the San Antonio Spurs."
  },
  {
    "objectID": "articles/public_health/covid_atx/covid_atx_analysis.html",
    "href": "articles/public_health/covid_atx/covid_atx_analysis.html",
    "title": "Analyzing COVID-19 in Austin",
    "section": "",
    "text": "Approaching the six-month mark of COVID-19 in Texas, one of the more common topics of discussion I hear is what the effects of coronavirus on our communities reveals about the world that existed before. And where we knew little about COVID-19 back in March, since that time a lot has been documented about what increases the risk of not only becoming severely ill, but also what increases the risk of being exposed to COVID-19 before you even get sick‚Äîsuch as having a job that requires you to be physically present for work (as opposed to working remotely from home) or not having access to a reliable internet connection.\nThat said, I want to explore some of these things in a series of blog posts‚Äìto see who is not only getting COVID, but also where folks are most at risk of being exposed to COVID and where they‚Äôre most at risk of not bouncing back. The trouble in answering these questions is that getting good data is difficult and, once you have it, organizing the data in a way that is useful and meaningful for others is even harder. So, I‚Äôm going to pace myself and search for datasets that can inform our conversation. For the purposes of these blog posts, I‚Äôm just going to focus on looking at the city-wide data in Austin‚Äîwhere I live‚Äîbecause I know all the data exists with regards to the questions I have pulled together. If you have questions, I invite you to ask them as well and I‚Äôll do my best to find data that can help provided some meaningful insight."
  },
  {
    "objectID": "articles/public_health/covid_atx/covid_atx_analysis.html#who-is-getting-covid-19-in-austin",
    "href": "articles/public_health/covid_atx/covid_atx_analysis.html#who-is-getting-covid-19-in-austin",
    "title": "Analyzing COVID-19 in Austin",
    "section": "Who is getting COVID-19 in Austin?",
    "text": "Who is getting COVID-19 in Austin?\nBefore we do any exploration of the more complicated questions identified, the best starting point would be exploring where COVID is occuring most in Austin. So let‚Äôs look at that before digging into other questions and start by pulling Austin Public Health‚Äôs data‚Äîwhich organizes cases at the zip code level and updated frequently‚Äîto map out cases. We don‚Äôt have access to where deaths occur most often, but cases at the zip code level is more than you can find in a lot of places. So we‚Äôll use what we have.\nHere‚Äôs where cases are occurring most often right now:\n\n\n\nThe first thing to notice is that cases are largely consolidated east of I-35. Without additional context, it‚Äôs worth wondering what role an interstate plays in shielding folks on one side from Coronavirus. With additional context about how the city was intentionally segregated through a ‚Äúmaster plan‚Äù developed in 1928 that used I-35 as a form of physical division, it‚Äôs worth paying attention to the dynamics of I-35 and including it as a geographical marker throughout future analyses.\n\nFor those unfamiliar, I would highly recommend reading through this analysis by the Austin Statesman on the role of I-35 in the city‚Äôs history of racial segregation, as a starting point. Other histories have been documented elsewhere, but this is a good starting point.\n\nIn the next few blog posts, I‚Äôll be writing about risk + resilience. Originally, I wanted to include everything here, but the charts and analysis became pretty extensive. For fear of exhausting everyone, including myself. We‚Äôll take these questions piece by piece.\nNext Blogpost: Who should be most worried about getting COVID?\nNote about this series: If you‚Äôre interested in asking specific questions to explore in this, reach out to me via Facebook or Twitter and I‚Äôll do my best to find data that can help provide some kind of meaningful answer.\nWhere can I find the code for this blog? Once I clean up the mess in this repo, I‚Äôll make it public on github. For now, I‚Äôm happy to share individual the Rmarkdown file used to produce each article.\n\n\nTLDR | Why not do this at the statewide level?\n\nOriginally, I considered focusing these articles at the statewide level for Texas, but I kept running into issues around finding data that could meaningfully answer some of my questions. The reasons for this are many, but some of the state‚Äôs existing COVID-19 data cannot be aggregated at anything beyond the statewide level for reasons outside of their control.\n\nExample - Test Positivity\nFor example, county-level test positivity is one of those things that has been difficult because COVID-19 lab testing has largely been decentralized, with over 97% of tests being conducted in commercial labs. Consequently, this means the state has to then coordinate data from over 4 million tests conducted outside of state testing labs.\n\n\n\n\n\n\n  \n    \n      Number of People Tested for SARS-CoV-2 in Texas, By Lab Type\n    \n    \n      Source: Texas Department of State Health Services | As of August 10th at 3:00PM CST1\n    \n  \n  \n    \n      \n      Location\n      Tests Processed\n    \n  \n  \n    \nTotal People Tested in Texas by Public Health Lab\n122.02K\n    \nNo. Tests by Commercial labs*\n4.49M\n    Total Tests\n‚Äî\n4,611,777\n  \n  \n    \n      *Unable to deduplicate figures for Commercial labs.\n    \n  \n  \n    \n      1 Data: From the 'Accessible Dashboard Data' File, under the 'Tests' tab.\n    \n  \n\n\n\n\nAnd given that these commercial labs aren‚Äôt actually part of a state agency, this means they have to learn new reporting rhythms and do work that, perhaps, they didn‚Äôt plan for or know they needed to do‚Äìsuch as keep all of the positives and negatives for each county, record those in a database by the county of origin, deduplicate them by the person who got the test, and then share all of that information back to the state in a manner that is uniform with all the other private labs in the state.\nThat said, not being able to compare how much a county is testing with how much they‚Äôre testing positive weakens a potential analysis and limits how much you can explore the dynamics between risk factors and things like test positivity.\n\n\nExample - Demographic Data\nAnother example of this information is demographic data about cases and fatalities. That information also doesn‚Äôt exist broken down by county‚Äìsometimes due to HIPAA requirements.\nSo if you wanted to analyze geographic COVID-19 trends between counties across multiple pieces of data such as demographic, known risk-factors, and other trends, you‚Äôd have a hard time accomplishing that because it currently doesn‚Äôt exist.\nFor that reason, I‚Äôve chosen to just focus on using city-level data given that cities tend to have better access to local information about their communities."
  },
  {
    "objectID": "articles/public_health/covid_atx/covid_atx_analysis.html#photo-credit",
    "href": "articles/public_health/covid_atx/covid_atx_analysis.html#photo-credit",
    "title": "Analyzing COVID-19 in Austin",
    "section": "Photo Credit",
    "text": "Photo Credit\n\nPhoto by Ryan Magsino on Unsplash"
  },
  {
    "objectID": "articles/rstats/mapping-in-r/index.html",
    "href": "articles/rstats/mapping-in-r/index.html",
    "title": "A Spatial/GIS Workflow for Interactive Maps in R",
    "section": "",
    "text": "At RStudio (Posit) Conference this year, I realized I‚Äôd been dealing with a bit of impostor syndrome while sitting at a Birds of a Feather table for folks interested in spatial data. As I sat there, I started asking people about their spatial work and had largely assumed people knew more than I did. In part, this is because I‚Äôve learned so many things from folks sharing through the #rstats hashtag on twitter and assumed people at the conference were just all farther down the line in the development of their skills than I was.\nThe reality was that most folks at my table were there because they were interested in learning how to work with Spatial Data in R and I was the only one at my table who had experience and background making maps, so folks asked for some resources and direction on how to get going with Spatial Data in R. For those at the table, I gave a very impromptu demo and walk through of core tools in my spatial workflow that a lot of people liked. Afterwards, some folks asked if I could share resources. So this post (or possibly series of posts?) is a first response to that.\nAt the core of my imposter syndrome was assuming folks already knew most of the stuff I did and, therefore, there wasn‚Äôt much of a need for me to share. So, at least with regards to spatial data, I‚Äôll try to be better about contributing to folks in the #rstats community online by sharing about different things. Anyway, here we go."
  },
  {
    "objectID": "articles/rstats/mapping-in-r/index.html#sharing-is-caring",
    "href": "articles/rstats/mapping-in-r/index.html#sharing-is-caring",
    "title": "A Spatial/GIS Workflow for Interactive Maps in R",
    "section": "",
    "text": "At RStudio (Posit) Conference this year, I realized I‚Äôd been dealing with a bit of impostor syndrome while sitting at a Birds of a Feather table for folks interested in spatial data. As I sat there, I started asking people about their spatial work and had largely assumed people knew more than I did. In part, this is because I‚Äôve learned so many things from folks sharing through the #rstats hashtag on twitter and assumed people at the conference were just all farther down the line in the development of their skills than I was.\nThe reality was that most folks at my table were there because they were interested in learning how to work with Spatial Data in R and I was the only one at my table who had experience and background making maps, so folks asked for some resources and direction on how to get going with Spatial Data in R. For those at the table, I gave a very impromptu demo and walk through of core tools in my spatial workflow that a lot of people liked. Afterwards, some folks asked if I could share resources. So this post (or possibly series of posts?) is a first response to that.\nAt the core of my imposter syndrome was assuming folks already knew most of the stuff I did and, therefore, there wasn‚Äôt much of a need for me to share. So, at least with regards to spatial data, I‚Äôll try to be better about contributing to folks in the #rstats community online by sharing about different things. Anyway, here we go."
  },
  {
    "objectID": "articles/rstats/mapping-in-r/index.html#first-question-can-you-do-any-spatialgis-work-in-r",
    "href": "articles/rstats/mapping-in-r/index.html#first-question-can-you-do-any-spatialgis-work-in-r",
    "title": "A Spatial/GIS Workflow for Interactive Maps in R",
    "section": "First Question: Can you do any Spatial/GIS Work in R?",
    "text": "First Question: Can you do any Spatial/GIS Work in R?\nYes."
  },
  {
    "objectID": "articles/rstats/mapping-in-r/index.html#second-question-are-r-tools-any-kind-of-good-for-sptialgis-workflows",
    "href": "articles/rstats/mapping-in-r/index.html#second-question-are-r-tools-any-kind-of-good-for-sptialgis-workflows",
    "title": "A Spatial/GIS Workflow for Interactive Maps in R",
    "section": "Second Question: Are R tools any kind of good for Sptial/GIS workflows?",
    "text": "Second Question: Are R tools any kind of good for Sptial/GIS workflows?\nAbsolutely. Positively. Definitively, yes. R is fan-freaking-tastic for a ton of random stuff, but it is especially amazing when it comes to spatial work. In fact, one of the reasons I‚Äôve come to love R so much is how simplified workflows are, regardless of how complicated the data may be.\nI often think back on a grad school GIS Course in 2017, where I first learned how to make maps. The workflows involved downloading separate spatial/numeric data files and required multiple pieces of software1 and hours of time to join those data, design the map, and export that map to a format you could use in a report or public-facing document. And that was really only static maps that end up in printed or PDF reports. Interactive workflows was an entirely different beast that required even more expensive software and other skills.\nIn contrast, consider the map below, which is built in this document. The code chunk at the very bottom of the document is what produces this map. It pulls the spatial and numeric data from its source (NCES Open Data) and visualizes it in about 10 lines of code. ü§Ø\nMore impressive is that, even though it contains over 100,000 school locations across the country, it‚Äôs very fast. Use your mouse/track-pad to hover the dots and zoom in-and-out."
  },
  {
    "objectID": "articles/rstats/mapping-in-r/index.html#how-you-do-this-in-r-feat.-rdeck",
    "href": "articles/rstats/mapping-in-r/index.html#how-you-do-this-in-r-feat.-rdeck",
    "title": "A Spatial/GIS Workflow for Interactive Maps in R",
    "section": "How you do this in R | Feat. {rdeck}\n",
    "text": "How you do this in R | Feat. {rdeck}\n\nTo do this in R, it‚Äôs pretty simple. First, you go to the website where the data lives. Ironically, it‚Äôs on ArcGIS‚Äôs publishing platform, which lets us know there‚Äôs too many data points to display without zooming in. The good news is you don‚Äôt need ArcGIS to use the data.\nAnyway, to use the data, you just click on the gold-colored ‚ÄúI want to use this‚Äù button at the bottom of the left-hand sidebar in the Step 1 screenshot. Once you‚Äôve clicked on that, copy the URL from the ‚ÄúGeoJSON‚Äù link under ‚ÄúView API Resources‚Äù.\n\n\n\n\nStep 1: Screenshot of NCES School Locations Data (Click to expand)\n\n\n\nStep 2: Screenshot of NCES School Locations Data (Click to expand)\n\n\n\nOnce you‚Äôve copied this code, you can read that spatial data directly into R with a function called read_sf() from the sf package.  Once you read that url into read_sf(), you can drop any empty spatial data from the geometry column (where all the spatial coordinates live for each dot) and then give the map a standard CRS to use across all dots.If you‚Äôre not familiar with sf, it‚Äôs what convinced me that, for the rest of my life, I never needed to pay for GIS software.\nHere‚Äôs what this looks like as code, and it‚Äôs just seven lines of code. You run this, you get the data, and now you‚Äôre ready to move on.\nlibrary(tidyverse) # Core R Tools\nlibrary(sf) # Core Spatial Package. Pretty much replaces ArcGIS for most spatial tasks.\nsf::sf_use_s2(FALSE)\n\nurl &lt;- \"https://services1.arcgis.com/Ua5sjt3LWTPigjyD/arcgis/rest/services/Public_School_Location_201819/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\"\n\ndata &lt;- read_sf(url) |&gt;\n  drop_na(geometry) |&gt;\n  st_as_sf(crs = st_crs(4326))\nAt this point, we are ready to map the data. Now, there‚Äôs lots of spatial packages out there to map with like {leaflet}, {mapview} , {mapdeck}, {tmap}, and {leafgl}, but I‚Äôve recently become a huge fan of an R package called {rdeck}. I‚Äôm partial to it because it‚Äôs incredibly lightweight and the maps look really great once you‚Äôve gotten a token from mapbox studio, even though you don‚Äôt necessarily need one. For more on getting a mapbox token, see Kyle Walker‚Äôs tutorial from his {mapboxapi} R package.\nSo how do we map in {rdeck}? Well, let‚Äôs pretend like you didn‚Äôt have a mapbox token. You could start by running a code chunk, like the one below, which has two functions from {rdeck}.\nThe first function, rdeck(...) is kind of like the ggplot() function. It creates a spatial canvas, which you can modify with a few arguments. In this case, we‚Äôve articulated that there‚Äôs no base map, but the overall theme of the map is ‚Äúlight‚Äù.\nThe second function, add_scatterplot_layer() is what we use to add the dots and define characteristics about them. In this case, it just adds the school dots, names the dot series ‚ÄúPublic Schools (SY21-22)‚Äù, and tells it where to get the dots from within our dataset (the column labeled geometry)\n\nShow the code | Build A Basic Maplibrary(rdeck) # Loads rdeck library\n\nrdeck(map_style = NULL,\n      theme = \"light\") |&gt;\n  add_scatterplot_layer(data = school_sites,\n                        name = \"Public Schools (SY21-22)\",\n                        get_position = geometry)\n\n\n\n\n\n\n\nHow can we improve this map?\nThe map above is a decent start, but there‚Äôs a couple of issues to resolve:\n\nThe dots kind of disappear as you zoom in.\nThe default bounding box is set more far out than I‚Äôd like.\nI also can‚Äôt really see any information about the schools.\nThe lack of a basemap may not be a problem for some maps, but for this one‚Ä¶ it‚Äôs just not great.\n\nTo fix this, we just add a few arguments to each of the two functions used in the previous code chunk.\nStyling our dots\nFirst, I‚Äôm going to style the dots. They look cool from far away, but appear to dissolve once you zoom in. This is the opposite of what we want if our goal is to let people explore the US and identify schools across the country. So, to fix this, we can use a couple of arguments within the existing add_scatterplot_layer() function.\nWe start by setting a minimum pixel size for the dots using radius_min_pixels. Leaving it blank will mean there‚Äôs no minimum as people zoom in. Setting it to 1 or 2 pixels will mean people will see a dot equivalent to those sizes no matter how far they zoom in.\nI‚Äôm also going to make the dots burnt orange, cause I‚Äôm a UT Austin alum ü§ò and think it‚Äôs a great color üòÇ. Lastly, I‚Äôm going to set the opacity of the dots. There‚Äôs some other arguments within the function you can play with, but I feel pretty good about these dots. They are visible and big enough to hover over with a mouse.\nAt this point, we only need to add a basemap, tooltip, and bounding box so the map doesn‚Äôt render so far out (leaving a lot of white space).\n\nShow the code | Styling The Dotsrdeck(map_style = NULL,\n      theme = \"light\") |&gt;\n  add_scatterplot_layer(data = school_sites,\n                        name = \"Public Schools (SY21-22)\",\n                        get_position = geometry,\n                        get_fill_color = \"#bf5700\",\n                        radius_min_pixels = 2,\n                        opacity = 0.3)\n\n\n\n\n\n\n\nAdding a tooltip\nAdding a tooltip is actually pretty easy and the pre-built tooltips are quite nice. In my code chunk, I‚Äôm going to adjust the existing data argument already set to our school_sites object and select only the Name and Location columns. They‚Äôre named differently, but I can rename them with school_sites |&gt; select(Name=NAME, Location = NMCBSA). After this, you add a tooltip argument to define what columns you want. A nice feature of the tooltip argument is that you can use tidy-select syntax to build your tooltip. Lastly, I add pickable = TRUE, which just means that the dots will respond as I move my mouse/trackpad around the map.\nIf you want, you can add some CSS afterwards, but it‚Äôs a little complicated to adjust and the default tooltips are easy to set and look good. Alright, onto basemaps and bounding boxes.\n\nShow the code | Adding A Tooltiprdeck(map_style = NULL,\n      theme = \"light\") |&gt;\n  add_scatterplot_layer(data = school_sites |&gt; select(Name=NAME, Location = NMCBSA),\n                        name = \"Public Schools (SY21-22)\",\n                        get_position = geometry,\n                        get_fill_color = \"#b57000\",\n                        radius_min_pixels = 2,\n                        opacity = 0.3,\n                        tooltip = c(Name, Location),\n                        pickable = TRUE)\n\n\n\n\n\n\n\nAdding a basemap\nAdding a basemap is easy once you get your token setup. Instead of setting map_style = NULL inside therdeck() function we‚Äôre currently, we just use one of the options from mapbox built into rdeck. For demonstration purposes, I‚Äôll use a style from mapbox called ‚ÄúLe Shine‚Äù by setting map_style = mapbox_gallery_le_shine(). In the original map, we use a style called ‚ÄúMinimo‚Äù, which I like and think works well to contrast basic geographic markers alongside the kind of dots we‚Äôre trying to visualize.\nNext up, bounding boxes!\n\nShow the code | Adding A Basemaprdeck(map_style = mapbox_gallery_le_shine(),\n      theme = \"light\") |&gt;\n  add_scatterplot_layer(data = school_sites |&gt; select(Name=NAME, Location = NMCBSA),\n                        name = \"Public Schools (SY21-22)\",\n                        get_position = geometry,\n                        get_fill_color = \"#b57000\",\n                        radius_min_pixels = 2,\n                        opacity = 0.3,\n                        tooltip = c(Name, Location),\n                        pickable = TRUE)\n\n\n\n\n\n\n\nAdding a bounding box\nHonestly, this one is kind of tricky and might be the least intuitive part of rdeck because the argument that controls this initial_bounds requires an object of st_bbox, sf, sfc class. The end result spits out a box of coordinates that looks like this:\n      xmin       ymin       xmax       ymax \n-106.64585   25.83706  -93.50782   36.50045 \nThe disclaimer here is that you don‚Äôt have to set a bounding box to adjust the view, but it certainly helps if you want to direct your audience to a specific location. I sort of hacked my way through this by piece-mealing a frankestein function of different things I found online called get_bbox(), largely based on Bob Rudis‚Äô {nominatim} package. I‚Äôve folded the code here, but feel free to look at if you want. The bottom line is it works, so just steal it and use it to get the bounding box you need for this argument.\n\nShow the code | Adding a Bounding Boxget_bbox &lt;- function(place) {\n\n  df_bbox &lt;- nominatim::bb_lookup(place)\n\n  btlr &lt;- df_bbox[1,c(\"bottom\", \"top\", \"left\", \"right\")]\n\n  v &lt;- as.numeric(btlr)\n  cmat &lt;- matrix(v[c(3, 3, 4, 4, 1, 2, 1, 2)], nrow = 4)\n  spobj &lt;- sp::SpatialPoints(coords = cmat)\n  sfobj &lt;- sf::st_as_sfc(spobj)\n  sf::st_crs(sfobj) &lt;- 4326\n  bbox &lt;- sf::st_bbox(sfobj)\n\n  bbox\n\n}\n\ntexas_bbox &lt;- get_bbox(\"Texas\")\n\n\nIn my case, I‚Äôm from Texas. I love Texas. And I want to focus the map on Texas. Not just cause I love it and am from here, but Texas is home to a significant chunk of America‚Äôs school-aged children. Of the reported 49.5 million children enrolled in US public schools, Texas accounts for over 5.4 million of them. So, I‚Äôm going to pass the result of my get_bbox() object into the initial_bounds argument of the rdeck() function, which will set the starting point to Texas.\n\nShow the code | Putting It All Togetherrdeck(map_style = mapbox_gallery_minimo(),\n      initial_bounds = texas_bbox,\n      theme = \"light\") |&gt;\n  add_scatterplot_layer(data = school_sites |&gt; select(Name=NAME, Location = NMCBSA),\n                        name = \"Public Schools (SY21-22)\",\n                        get_position = geometry,\n                        get_fill_color = \"#bf5700\",\n                        radius_min_pixels = 2,\n                        opacity = 0.3,\n                        tooltip = c(Name, Location),\n                        pickable = TRUE\n)\n\n\n\n\n\n\n And that‚Äôs it! If you have any questions, revisions, or suggestions, just shoot me a message on twitter."
  },
  {
    "objectID": "articles/rstats/mapping-in-r/index.html#acknowledgments",
    "href": "articles/rstats/mapping-in-r/index.html#acknowledgments",
    "title": "A Spatial/GIS Workflow for Interactive Maps in R",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nPhoto by Andrew Neel on Unsplash"
  },
  {
    "objectID": "articles/rstats/mapping-in-r/index.html#footnotes",
    "href": "articles/rstats/mapping-in-r/index.html#footnotes",
    "title": "A Spatial/GIS Workflow for Interactive Maps in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nIt‚Äôs worth mentioning that some of the software required for the described GIS workflows requires using Windows and costs a ton of money.‚Ü©Ô∏é"
  },
  {
    "objectID": "articles/school_finance/all_about_racism/all_about_racism.html",
    "href": "articles/school_finance/all_about_racism/all_about_racism.html",
    "title": "ICYMI: This was all about racism",
    "section": "",
    "text": "This is the fourth and final installment in a series of essays on the legacy of Rodriguez v. San Antonio ISD ‚Äî a court battle that began in San Antonio and ended in the Supreme Court ‚Äî and determined that American children have no constitutional right to an education. Read the first, second and third essays.\nIn 1968, Bexar County Commissioner A.J. Plough sat down with CBS News to discuss the plight of Latinos living in hunger and poverty on San Antonio‚Äôs West Side.\nPlough: ‚ÄúWell, why are they not getting enough food? Because the father won‚Äôt work. And I mean, won‚Äôt work. If they won‚Äôt work, do you expect the taxpayer to raise all the kids ‚Ä¶ ‚Äôcause their daddies won‚Äôt work? First, let‚Äôs do something with the daddies and then, yes, take care of the kids.\nThe reporter asked how chronically hungry students could be expected to learn on an empty stomach.\n‚ÄúWell, what do you mean learn properly in school? Do you really need school‚Ä¶ other than, say, an eighth grade education?‚Äù Plough said, later adding, ‚ÄúPeople keep talking about this education, ‚Äòa college education.‚Äô It‚Äôs not necessary.‚Äù\nMeanwhile, the same disparities that prompted the CBS interview caught the attention of the United States Commission on Civil Rights, which held hearings that same year in San Antonio to collect information regarding civil rights violations against Mexican-American families in the Southwest. What they found reveals how racism shaped society in the 20th century here and across the United States.\nIn total, the commission spent six months in San Antonio and interviewed more than 1,000 residents to understand these challenges in the context of housing, education, employment, voting and justice.\nContrary to the analysis of Plough and others, the Commission found that Hispanic parents desperately wanted good jobs, their children wanted to earn college degrees, and both wanted to eat three meals a day in decent housing. However, the education system and public infrastructure failed them. Parents faced discrimination in the workplace. Their upward mobility was further restricted by exclusionary real estate and lending practices. Using racially restrictive housing covenants white residents prohibited Black and Hispanic families from purchasing homes in multiple neighborhoods across the city. Such was unvarnished racism in mid-century America. Discrimination was not problematic, and equity was not a priority.\nAs a result of these attitudes, economic development, which increased property values and neighborhood resources, was unevenly distributed. Until 1977, San Antonio‚Äôs City Council was entirely made up of members who could live anywhere in the city, and the Northside was most heavily represented. Major investments like the South Texas Medical Center and the University of Texas at San Antonio directed the city‚Äôs continued growth to the north. Meanwhile, the parts of town where black and Hispanic families were contained ‚Äî the East, West and South Sides ‚Äî were further devalued by neglect of aging and absent infrastructure.\nPolitical, economic and social exclusion worked in concert to create an intractable disadvantage that still clings to the heels of ambitious and bright Hispanic students decades later. While few in 2017 would espouse overt racism, the damning testimony recorded by the Civil Rights commission has yet to be fully rectified.\nFigure from Folo Media. The Thiry Fine Arts Building at Our Lady of the Lake University was the site of the 1968 hearings of the United States Commission on Civil Rights. Tomas Gonzalez / Special to Folo Media"
  },
  {
    "objectID": "articles/school_finance/all_about_racism/all_about_racism.html#acknowledgments",
    "href": "articles/school_finance/all_about_racism/all_about_racism.html#acknowledgments",
    "title": "ICYMI: This was all about racism",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was produced during my time at Folo Media during 2017. Folo Media was a nonprofit newsroom focused on the challenges of inequity and neighborhood segregation in San Antonio, Texas. The newsroom ran from the spring of 2017 through early 2018, and during that span published more than 100 stories covering a range of issues, such as: housing, education, racial segregation, non-profit solutions, and much more.\nIf you are interested in following the work of those who helped lead Folo Media‚Äôs work, I encourage you to keep up with the American Journalism Project, the H.E.Butt Foundation‚Äôs Echoes Magazine, and the San Antonio Heron."
  },
  {
    "objectID": "articles/school_finance/how_sa_segregated_schools/how_sa_segregated_schools.html",
    "href": "articles/school_finance/how_sa_segregated_schools/how_sa_segregated_schools.html",
    "title": "How San Antonio segregated its schools",
    "section": "",
    "text": "Note: This story was originally published by Folo Media.\nThis is the second installment in a series of essays on the legacy of a court battle that began in San Antonio, ended in the Supreme Court, and determined that American children have no constitutional right to an education. Read the opening essay here.\nIn San Antonio Independent School District v. Rodriguez, Demetrio Rodriguez‚Äôs allegation of bias based on wealth was built on decades of injustice that neither began nor ended at the schoolhouse doors.\nHad the Supreme Court decided in Rodriguez‚Äôs favor, it might have raised questions about how property values came to be so unevenly distributed across San Antonio. The way communities built the ‚Äúwealth‚Äù that gave them such an advantage in the Texas school finance formula was the result of some of San Antonio‚Äôs most deadly public policy decisions.\nFigure from Folo Media. In this 1949 photo, children play near outdoor toilets in ‚Äúslum-corrals‚Äù on San Antonio‚Äôs West Side. In these particular shanties, built in 1913, six toilets and one shower were shared by more than 100 residents. COURTESY THE DOLPH BRISCOE CENTER FOR AMERICAN HISTORY / THE UNIVERSITY OF TEXAS AT AUSTIN"
  },
  {
    "objectID": "articles/school_finance/how_sa_segregated_schools/how_sa_segregated_schools.html#acknowledgments",
    "href": "articles/school_finance/how_sa_segregated_schools/how_sa_segregated_schools.html#acknowledgments",
    "title": "How San Antonio segregated its schools",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was produced during my time at Folo Media during 2017. Folo Media was a nonprofit newsroom focused on the challenges of inequity and neighborhood segregation in San Antonio, Texas. The newsroom ran from the spring of 2017 through early 2018, and during that span published more than 100 stories covering a range of issues, such as: housing, education, racial segregation, non-profit solutions, and much more.\nIf you are interested in following the work of those who helped lead Folo Media‚Äôs work, I encourage you to keep up with the American Journalism Project, the H.E.Butt Foundation‚Äôs Echoes Magazine, and the San Antonio Heron."
  },
  {
    "objectID": "articles/school_finance/public-ed-testimony/public_ed_testimony.html",
    "href": "articles/school_finance/public-ed-testimony/public_ed_testimony.html",
    "title": "My Testimony to The Texas Commission on Public School Finance",
    "section": "",
    "text": "After almost 10 hours waiting to testify before the Texas Commission on Public School Finance, I took full advantage of the three minutes I had to tell them about the dramatic rise of school poverty, reduced state funding to public education, the lack of transparency in equity funding that hardly ends up where it should go, and the need to put updated funding weights for the cost of education and special populations on a mandated review schedule similar to what the state does with agencies and the sunset commission.\nHere is a modified copy of my testimony."
  },
  {
    "objectID": "articles/school_finance/public-ed-testimony/public_ed_testimony.html#transcript-of-my-testimony",
    "href": "articles/school_finance/public-ed-testimony/public_ed_testimony.html#transcript-of-my-testimony",
    "title": "My Testimony to The Texas Commission on Public School Finance",
    "section": "Transcript of My Testimony",
    "text": "Transcript of My Testimony\nMy name is Matt Worthington and I am a resident of Austin, TX; parent of two girls; former special education teacher; and a former district technology administrator of DC Public Schools and KIPP Austin Public Schools who holds a Masters in Education and am nearing completion of a second Master‚Äôs in Public Affairs from the LBJ School at the University of Texas.\nMembers of the committee: Texas schools are getting poorer faster than they‚Äôre getting bigger. Between 1995 & 2016, the share of students identified as economically disadvantaged in traditional ISDs rose by almost 13%1. In raw numbers, that translates to an 83% increase in the total number of economically disadvantaged students from 1995 to 2016. This is because the increase of poor students is outpacing the overall population growth of students.2\nThe current body of medical & economic research on child poverty tells us that poverty exposes children to what is called ‚Äútoxic stress‚Äù. This adversity has strong linear relationships with multiple health impairments in adulthood, including heart disease, obesity, cancer, alcoholism, and depression.3 They reduce life expectancies for children. Counter that, one of the more significant explanations for long life-expectancies is local government spending.4\nIn Texas, school poverty disproportionately effects students of color. In 2014, almost half of the non-white student population in Texas attended high-poverty schools, which generally have greater academic, social, and emotional needs but receive inadequate funding for programming.5\nMeanwhile, the state has lowered its share of per-pupil funding on traditional ISDs. Since 2008, the state‚Äôs funding has dipped by almost $500 per ADA while local property tax funding has skyrocketed at over $800 per ADA. Multiplied by the respective ADA counts of students in 2016, this shift constitutes an increased burden of roughly $4.3 billion for statewide local property tax funding and a decreased responsibility of roughly $2.5 billion for funding from the state.6\nData: Texas Education Agency - PEIMS District Financial Actual Reports; accessed via a public information request. \nWhile recapture was designed to provide equity, it has been undermined by the state‚Äôs supplanting of revenues that it claims to supplement. Detailed records of recapture revenues show an over 800% increase in annual collections since 1994, but the state maintains no accounting method that tracks exactly7 where recapture money goes, by district. In other words, the public cannot see where the money ends up.\nIncreasingly, poor districts are running out of ways to get more revenue. Nearly 43% of property poor districts have maxed out on their M&O rates compared to only 14% of property wealthy districts.8 This means poor districts are increasingly reliant on the hope for more funding from recapture. However, this hope is fragile because most property wealthy districts tax on M&O within their range of golden pennies, which is not subject to recapture.9 This is because many property wealthy districts do not trust the state as a fiduciary of recapture revenues.\nIf we fail to ensure that all 5.3 million children in Texas attend schools that are adequately and transparently funded, we risk a future where Texas is characterized by its lack moral, economic, or cultural wealth. That will happen if we fail to ensure that money intended for poor children actually makes its way to those classrooms where they are learning against all odds. If that means we, as Texas taxpayers, must pay more money than so be it. The lives of 5.3 million children and future generations of Texans literally depend upon it.\nThank you."
  },
  {
    "objectID": "articles/school_finance/public-ed-testimony/public_ed_testimony.html#photo-credit",
    "href": "articles/school_finance/public-ed-testimony/public_ed_testimony.html#photo-credit",
    "title": "My Testimony to The Texas Commission on Public School Finance",
    "section": "Photo Credit",
    "text": "Photo Credit\n\nPhoto by Meggyn Pomerleau on Unsplash"
  },
  {
    "objectID": "articles/school_finance/public-ed-testimony/public_ed_testimony.html#footnotes",
    "href": "articles/school_finance/public-ed-testimony/public_ed_testimony.html#footnotes",
    "title": "My Testimony to The Texas Commission on Public School Finance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nData: Texas Education Agency - Annual Snapshot School District Profiles, 1995 & 2016.‚Ü©Ô∏é\nI use the phrase ‚Äúpoor children‚Äù or ‚Äúin poverty‚Äù here in reference to students who are economically disadvantaged. While some are quick to note that the Federal Poverty Guidelines are not the same thing as economically disadvantaged (which follows guidelines from the National School Lunch & Breakfast Programs), it‚Äôs worth noting that the Federal Poverty Guideline is often criticized for being a very low measure of poverty in 2018. For a family of four, the 2018 Federal Poverty Guideline is $25,100. This figure is before taxes, which would further reduce the take home pay. The numbers used to determine economically disadvantaged are either 130% (the free-lunch population) or 180% (the reduced-lunch population) of the Federal Poverty Guideline, which are far lower than the 200% measure used by many poverty research institutions like Spotlight on Poverty & Opportunity: The Source for News, Ideas and Action.‚Ü©Ô∏é\nShonkoff, J. P. (2016). Capitalizing on Advances in Science to Reduce the Health Consequences of Early Childhood Adversity. JAMA Pediatrics, 170(10), 1003. https://doi.org/10.1001/jamapediatrics.2016.1559‚Ü©Ô∏é\nChetty, R., Stepner, M., Abraham, S., Lin, S., Scuderi, B., Turner, N., ‚Ä¶ Cutler, D. (2016). The Association Between Income and Life Expectancy in the United States, 2001-2014. JAMA, 315(16), 1750. https://doi.org/10.1001/jama.2016.4226‚Ü©Ô∏é\nSource: The National Equity Atlas; Data: the National Center for Education Statistics (NCES) Common Core of Data (CCD) Public Elementary/Secondary School Universe Survey.‚Ü©Ô∏é\nData: Texas Education Agency - PEIMS District Financial Actual Reports; accessed via a public information request.‚Ü©Ô∏é\nIn an interview with Folo Media this past summer, the office of State Funding for the Texas Education Agency verified that their current method of finance does not allow them to keep track of how much schools receive in recapture funds.‚Ü©Ô∏é\nData: Texas Education Agency - State Aid Template 2017-18 and 2018-19 Biennium; accessed online.‚Ü©Ô∏é\nData: Texas Education Agency - State Aid Template 2017-18 and 2018-19 Biennium; accessed online.‚Ü©Ô∏é"
  },
  {
    "objectID": "articles/school_finance/why_americans_have_no_right_to_education/why_americans_have_no_right_to_education.html",
    "href": "articles/school_finance/why_americans_have_no_right_to_education/why_americans_have_no_right_to_education.html",
    "title": "Why Americans Have No Right To An Education",
    "section": "",
    "text": "Note: This story was originally published by Folo Media.\nIn case you missed it: Matt Worthington joined State Rep.¬†Diego Bernal and SAISD‚Äôs Chief Innovation Officer Mohammad Choudhury on TPR‚Äôs The Source to discuss school finance in San Antonio. Listen to their conversation here.\nForty-four years ago, the United States Supreme Court ruled that children have no fundamental right to receive an education. If you never knew that, you aren‚Äôt alone ‚Äî people are surprised when they discover this little-known piece of American history.\nIf you‚Äôre from San Antonio, you may be further surprised to know that this history was born right here, in the city‚Äôs West Side.\nRoe v. Wade got all the headlines in 1973, but just three months after that landmark decision, the Supreme Court reached another landmark 5-4 decision: San Antonio Independent School District v. Rodriguez, in which a group of San Antonio families from the Edgewood community lost their years-long fight for equal education. The court‚Äôs majority found that the school finance system, which was based on property taxes and resulted in stark inequalities between schools, was not in violation of the Fourteenth Amendment‚Äôs equal protection clause. Education finance has been re-litigated multiple times at the state level, but it remains one of the key legal reasons that extreme inequalities persist in American schools today.\nThough I was born and raised in San Antonio, I didn‚Äôt become familiar with the Rodriguez case until after I moved to Washington, D.C. in 2009 and began teaching students with disabilities in DC Public Schools. After landing the job, I learned the school had been reconstituted, which meant the district wanted to start over from scratch. Except the only thing replaced was the staff. The archaic facilities and its existing resources were still mostly outdated, incomplete or unorganized. Depending on the classroom, any number of concerns could be found ‚Äî broken phones, disconnected intercoms, overhead projectors with no light bulbs, computers with tattered keyboards, and ceilings that dripped with bad weather. And then there was our disheveled library, repeated pest problems, and occasional breaks in water lines that left us without clean water for hours at a time.\nWhen I learned about the Rodriguez case, a wave of shock and depression washed over me as I realized that schools like mine were deemed legally acceptable learning environments for children. More difficult was understanding that this reality spanned across multiple decades, starting with a district from my hometown in a case I had never heard about growing up.\nThe case‚Äôs roots trace back to spring 1968, when Mexican-American families from the Edgewood District Concerned Parents Association hired a lawyer named Arthur Gochman to investigate legal action regarding a number of concerns they had with the conditions their children were forced to endure. Those children felt the concerns, too, and on May 16, 1968, 400 Edgewood High School students walked out at the 10 a.m. bell. Their protest called attention to antiquated curricula, the mistreatment of students, abysmal facilities, and consistently poor funding.\nFigure from Folo Media\nThe Edgewood families‚Äî‚Äìrepresented by Demetrio Rodriguez (pictured above), who worked at Kelly Air Force Base and had children attending Edgewood schools‚Äîclaimed that the Texas school funding methods, which are determined by property taxes, were responsible for Edgewood‚Äôs inferior quality as a district. Low property values doomed kids to bad schools, so Edgewood was powerless to change its circumstances.\nThe State of Texas responded by claiming it provided ‚Äúa minimum‚Äù education, which was all they were legally required to do. In the oral arguments, Gochman pressed the court to consider what was at stake if the court ruled in favor of ‚Äúminimums‚Äù and against the Edgewood families.\nAmerica, he said, risked endorsing a dual-class society where some have access to real opportunity and others do not.\nGochman argued passionately on behalf of poor children living in the most resource-deprived school districts of San Antonio. According to news reports of the trial, a victory for Rodriguez and the Edgewood families seemed almost inevitable to those monitoring it closely. Governors and other state legislators braced for what Justice Lewis Powell later said would have been ‚Äúan unprecedented upheaval in public education.‚Äù As the New York Times reported at the time: ‚ÄúEvery state has studies under way, and many have passed or are considering legislation to eliminate‚Äù the fundamental inequalities in the U.S. system of school finance.\nBut the disruption never came. On March 21, 1973, the court ruled against Demetrio Rodriguez and the other Mexican American families from Edgewood. Education, they found, is not explicitly enumerated in the constitution as a right, so it could not be protected under the Fourteenth Amendment.\nYet, even as it denied equality in education, the court said it highly valued education. In the majority opinion written by Justice Powell, the court reaffirmed its contention from Brown v. Board of Education that ‚Äúeducation is perhaps the most important function of state and local governments.‚Äù The court even described the unjust character of America‚Äôs education system. It never reasoned that the way the U.S. funds education was fair or sensible.\nAs Justice Potter Stewart wrote, ‚ÄúThe method of financing public schools in Texas, as in almost every other State, has resulted in a system of public education that can fairly be described as chaotic and unjust. It does not follow, however, and I cannot find, that this system violates the Constitution of the United States.‚Äù\nThe American education system was not fair. It was merely constitutional.\nSince 1973, we have tried to fix education in many ways. We have tried fixing standards, teachers, school leaders, students, parents, tests, and everything in between. Two fixes we‚Äôve never called for: Amending the constitution to protect the right to an education, and transforming school finance systems that courts since Rodriguez have continued to condemn. To this day, in case after case, courts call our education finance system ‚Äúunjust,‚Äù ‚ÄúByzantine,‚Äù and ‚Äúawful‚Äù ‚Äî but not unconstitutional.\nI‚Äôve spent the last few months exploring the legacy of the Rodriguez case. It‚Äôs a famous case within education circles, but has not received anything near the attention of, say, Roe v. Wade. Even most people from San Antonio have likely never heard of it, just like me until a few years back. But as I‚Äôve learned, both the history and after-effects of Rodriguez are extremely illuminating. The story of Rodriguez has a lot to teach us. Each of those events or conversations provides insight into the present vulnerabilities felt across Texas and San Antonio.\nFigure From Folo Media. Demetrio Rodriguez, plaintiff in the San Antonio Independent School District v. Rodriguez school finance case, speaks to Dr.¬†Thomas Cleaver‚Äôs graduate seminar on community/school relations during the summer of 1973. GIL BARRERA / COURTESY UTSA SPECIAL COLLECTIONS.\nIn the weeks ahead, I‚Äôll be writing a series of essays about the Texas school finance system. It‚Äôs complicated stuff, but this issue is essential because of how it directly impacts so many people across Texas. My aim is to make the issue accessible ‚Äî and to show how this ‚Äúchaotic and unjust‚Äù system shapes the deep inequalities of possibility that threaten countless families and children today.\nNote: Photos of Demetrio Rodriguez are provided by Gil Barrera / Courtesy UTSA Special Collections"
  },
  {
    "objectID": "articles/school_finance/why_americans_have_no_right_to_education/why_americans_have_no_right_to_education.html#acknowledgments",
    "href": "articles/school_finance/why_americans_have_no_right_to_education/why_americans_have_no_right_to_education.html#acknowledgments",
    "title": "Why Americans Have No Right To An Education",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was produced during my time at Folo Media during 2017. Folo Media was a nonprofit newsroom focused on the challenges of inequity and neighborhood segregation in San Antonio, Texas. The newsroom ran from the spring of 2017 through early 2018, and during that span published more than 100 stories covering a range of issues, such as: housing, education, racial segregation, non-profit solutions, and much more.\nIf you are interested in following the work of those who helped lead Folo Media‚Äôs work, I encourage you to keep up with the American Journalism Project, the H.E.Butt Foundation‚Äôs Echoes Magazine, and the San Antonio Heron."
  },
  {
    "objectID": "docs/tutorials/index.html",
    "href": "docs/tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "::: {.grid .step .column-page-right} ::: {.g-col-lg-3 .g-col-12} ## Step 1\n\nInstall Quarto\n:::\n::: {.g-col-lg-9 .g-col-12}"
  }
]